seeds: [42,43,44]

pooling: attention
opt:
  lr: 1e-3
  epochs: 20
  batch_size: 128

normalize:
  mean: [0.1307]
  std: [0.3081]

model:
  pool_topk: 0.7
  proj_in_channels: 150
  conv1_kernel_size: 9
  pretrained_filter_path: "/home/tonyhuh/again_0716/filters_emnist_pct5_k150_9_wosobel.npy"
  freeze_conv1: False
  proj_out_dim: 32
  num_heads: 4
  global_topk_ratio: 0.08

  # === MoE (4 experts) ===
  num_experts: 4
  expert_hidden: 128
  top_k: 4  # 안정화 후 2로 줄여 sparsity 실험 가능

  gating:
    type: vanilla
    lambda_load: 0.003
    lambda_ent: 0.0003

  ohem:
    use_ohem: true
    keep_ratio: 0.6

  num_classes: 47

# Freeze 스케줄(초기 몇 epoch 동안 고정)
schedule:
  freeze_trunk_epochs: 3
  freeze_gate_epochs: 3

# 초기화 입력(K2 스크립트 인자에서도 전달)
init:
  k0_ckpt: ""      # tools/k2_train_moe.py 실행시 --k0_ckpt 로 전달 권장
  gate_ckpt: ""    # tools/k2_train_moe.py 실행시 --gate_ckpt 로 전달 권장
  add_expert_noise_std: 0.0  # 전문가 복제 후 노이즈(선택)