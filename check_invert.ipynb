{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47025e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned 0/112800, current inverted count 0\n",
      "Scanned 5000/112800, current inverted count 167\n",
      "Scanned 10000/112800, current inverted count 353\n",
      "Scanned 15000/112800, current inverted count 533\n",
      "Scanned 20000/112800, current inverted count 718\n",
      "Scanned 25000/112800, current inverted count 911\n",
      "Scanned 30000/112800, current inverted count 1100\n",
      "Scanned 35000/112800, current inverted count 1267\n",
      "Scanned 40000/112800, current inverted count 1467\n",
      "Scanned 45000/112800, current inverted count 1659\n",
      "Scanned 50000/112800, current inverted count 1859\n",
      "Scanned 55000/112800, current inverted count 2043\n",
      "Scanned 60000/112800, current inverted count 2229\n",
      "Scanned 65000/112800, current inverted count 2408\n",
      "Scanned 70000/112800, current inverted count 2604\n",
      "Scanned 75000/112800, current inverted count 2800\n",
      "Scanned 80000/112800, current inverted count 2952\n",
      "Scanned 85000/112800, current inverted count 3118\n",
      "Scanned 90000/112800, current inverted count 3275\n",
      "Scanned 95000/112800, current inverted count 3458\n",
      "Scanned 100000/112800, current inverted count 3649\n",
      "Scanned 105000/112800, current inverted count 3839\n",
      "Scanned 110000/112800, current inverted count 4016\n",
      "Total inverted on train set: 4108/112800 (3.642%)\n"
     ]
    }
   ],
   "source": [
    "# diagnose_invert_smart.py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import EMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Reuse the same transform logic classes (copy from your train_expert_analyze_h.py)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SobelGrayTransform:\n",
    "    def __init__(self):\n",
    "        kx = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=torch.float32)\n",
    "        ky = torch.tensor([[-1,-2,-1],[0,0,0],[1,2,1]], dtype=torch.float32)\n",
    "        self.fx = kx.view(1,1,3,3)\n",
    "        self.fy = ky.view(1,1,3,3)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        # expects tensor [1,H,W] in [0,1]\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            if tensor.ndim == 2:\n",
    "                t = tensor.unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
    "            elif tensor.ndim == 3 and tensor.shape[0] == 1:\n",
    "                t = tensor.unsqueeze(0)  # [1,1,H,W]\n",
    "            else:\n",
    "                t = tensor.unsqueeze(0)\n",
    "        else:\n",
    "            raise TypeError(f\"SobelGrayTransform expected tensor, got {type(tensor)}\")\n",
    "        gx = F.conv2d(t, self.fx.to(t.device), padding=1)\n",
    "        gy = F.conv2d(t, self.fy.to(t.device), padding=1)\n",
    "        mag = (gx**2 + gy**2).sqrt().squeeze(0).squeeze(0)  # (H,W)\n",
    "        minv = mag.min()\n",
    "        maxv = mag.max()\n",
    "        mag = (mag - minv) / (maxv - minv + 1e-6)\n",
    "        return mag.unsqueeze(0)  # [1,H,W]\n",
    "\n",
    "class InvertIfSmart:\n",
    "    def __init__(self, border_width=8, otsu_threshold=True, sobel_transform=None, sobel_thresh=0.3):\n",
    "        self.border_width = border_width\n",
    "        self.otsu_enabled = otsu_threshold\n",
    "        self.sobel = sobel_transform\n",
    "        self.sobel_thresh = sobel_thresh\n",
    "        self.last_inverted = False\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # img: PIL.Image or torch.Tensor in [0,1]\n",
    "        if isinstance(img, Image.Image):\n",
    "            arr = np.array(img).astype(np.float32) / 255.0  # H,W\n",
    "            tensor = torch.from_numpy(arr).unsqueeze(0)  # [1,H,W]\n",
    "        elif isinstance(img, torch.Tensor):\n",
    "            tensor = img.clone()\n",
    "            if tensor.ndim == 2:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            elif tensor.ndim == 3 and tensor.shape[0] != 1:\n",
    "                tensor = tensor.mean(dim=0, keepdim=True)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported type {type(img)} for InvertIfSmart\")\n",
    "\n",
    "        _, H, W = tensor.shape\n",
    "        bw = min(self.border_width, H//2, W//2)\n",
    "\n",
    "        top_border    = tensor[:, :bw, :].reshape(-1)\n",
    "        bottom_border = tensor[:, H-bw:, :].reshape(-1)\n",
    "        left_border   = tensor[:, :, :bw].reshape(-1)\n",
    "        right_border  = tensor[:, :, W-bw:].reshape(-1)\n",
    "        border_pixels = torch.cat([top_border, bottom_border, left_border, right_border], dim=0)\n",
    "        center_pixels = tensor[:, bw:H-bw, bw:W-bw].reshape(-1)\n",
    "\n",
    "        border_mean = float(border_pixels.mean().item())\n",
    "        center_mean = float(center_pixels.mean().item())\n",
    "\n",
    "        out = tensor\n",
    "        inverted = False\n",
    "\n",
    "        # primary signal: border brighter than center\n",
    "        if border_mean > center_mean:\n",
    "            out = 1.0 - tensor\n",
    "            inverted = True\n",
    "        else:\n",
    "            # secondary: Otsu suggests inversion AND Sobel is weak\n",
    "            otsu_invert = False\n",
    "            if self.otsu_enabled:\n",
    "                img_np = (tensor.squeeze(0).cpu().numpy() * 255.0).astype(np.uint8)\n",
    "                _, bin_img = cv2.threshold(img_np, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "                num_white = int((bin_img == 255).sum())\n",
    "                num_black = int((bin_img ==   0).sum())\n",
    "                if num_white > num_black:\n",
    "                    otsu_invert = True\n",
    "\n",
    "            sobel_weak = False\n",
    "            if self.sobel is not None:\n",
    "                sobel_map = self.sobel(tensor)\n",
    "                mean_sobel = float(sobel_map.mean().item())\n",
    "                if mean_sobel < self.sobel_thresh:\n",
    "                    sobel_weak = True\n",
    "\n",
    "            if otsu_invert and sobel_weak:\n",
    "                out = 1.0 - tensor\n",
    "                inverted = True\n",
    "\n",
    "        self.last_inverted = inverted\n",
    "        return out  # [1,H,W]\n",
    "\n",
    "def show_pair(orig, processed, out_path, title_suffix=\"\"):\n",
    "    o = orig.squeeze(0).numpy()\n",
    "    p = processed.squeeze(0).numpy()\n",
    "    fig, axs = plt.subplots(1,2, figsize=(4,2))\n",
    "    axs[0].imshow(o, cmap='gray', vmin=0, vmax=1); axs[0].set_title('Original')\n",
    "    axs[1].imshow(p, cmap='gray', vmin=0, vmax=1); axs[1].set_title('After InvertIfSmart')\n",
    "    for ax in axs: ax.axis('off')\n",
    "    plt.suptitle(title_suffix)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "def main():\n",
    "    os.makedirs(\"invert_diagnosis\", exist_ok=True)\n",
    "\n",
    "    # dataset without any transform to get raw PILs\n",
    "    ds = EMNIST('data', split='balanced', train=True, download=True, transform=None)\n",
    "    sobel = SobelGrayTransform()\n",
    "    inverter = InvertIfSmart(sobel_transform=sobel, sobel_thresh=0.3)\n",
    "\n",
    "    total = len(ds)\n",
    "    inverted_indices = []\n",
    "    for i in range(total):\n",
    "        pil_img, label = ds[i]\n",
    "        out_tensor = inverter(pil_img)  # [1,H,W]\n",
    "        if inverter.last_inverted:\n",
    "            inverted_indices.append(i)\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"Scanned {i}/{total}, current inverted count {len(inverted_indices)}\")\n",
    "\n",
    "    print(f\"Total inverted on train set: {len(inverted_indices)}/{total} ({len(inverted_indices)/total:.3%})\")\n",
    "\n",
    "    # sample up to 10 inverted examples for inspection\n",
    "    sample_idxs = inverted_indices[:10]\n",
    "    for rank, idx in enumerate(sample_idxs):\n",
    "        pil_img, label = ds[idx]\n",
    "        orig_tensor = transforms.ToTensor()(pil_img)  # [1,H,W]\n",
    "        inverter(pil_img)\n",
    "        processed_tensor = inverter(pil_img)  # note: calling again to get the processed output\n",
    "        show_pair(orig_tensor, processed_tensor,\n",
    "                  out_path=f\"invert_diagnosis/invert_sample_{rank}_idx_{idx}_lbl_{label}.png\",\n",
    "                  title_suffix=f\"idx={idx} label={label} inverted={inverter.last_inverted}\")\n",
    "\n",
    "    # also save stats\n",
    "    with open(\"invert_diagnosis/summary.txt\", \"w\") as f:\n",
    "        f.write(f\"total={total}\\n\")\n",
    "        f.write(f\"inverted={len(inverted_indices)}\\n\")\n",
    "        f.write(f\"ratio={len(inverted_indices)/total:.4f}\\n\")\n",
    "        f.write(\"sample_indices=\" + \",\".join(str(i) for i in sample_idxs) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
